import warnings
import torch
import torch.optim as optim
from accelerate import Accelerator
from torch.utils.data import DataLoader
from torchmetrics.functional import peak_signal_noise_ratio, structural_similarity_index_measure
from torchmetrics.functional.regression import mean_squared_error
from tqdm import tqdm
import matplotlib.pyplot as plt
import os

from config import Config
from data import get_data
from models import *
from utils import *
from utils import losses
from utils.loss_scheduler import LossWeightScheduler, create_loss_scheduler_from_config

warnings.filterwarnings('ignore')


def seed_everything(seed):
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def manage_best_checkpoints(best_checkpoints, current_rmse, current_epoch, model_session, save_dir, max_keep=2):
    """
    ç®¡ç†æœ€å¥½çš„æƒé‡æ–‡ä»¶ï¼Œåªä¿ç•™æœ€å¥½çš„ä¸¤ä¸ª
    Args:
        best_checkpoints: å½“å‰æœ€å¥½çš„checkpointsåˆ—è¡¨ [(rmse, epoch, filepath), ...]
        current_rmse: å½“å‰epochçš„RMSE
        current_epoch: å½“å‰epoch
        model_session: æ¨¡å‹ä¼šè¯å
        save_dir: ä¿å­˜ç›®å½•
        max_keep: æœ€å¤šä¿ç•™çš„checkpointæ•°é‡
    Returns:
        updated_best_checkpoints: æ›´æ–°åçš„æœ€å¥½checkpointsåˆ—è¡¨
        should_save: æ˜¯å¦åº”è¯¥ä¿å­˜å½“å‰checkpoint
    """
    should_save = False
    
    # å¦‚æœè¿˜æ²¡æœ‰è¾¾åˆ°æœ€å¤§ä¿ç•™æ•°é‡ï¼Œç›´æ¥æ·»åŠ 
    if len(best_checkpoints) < max_keep:
        should_save = True
    else:
        # æ‰¾åˆ°å½“å‰æœ€å·®çš„checkpoint
        worst_rmse = max(best_checkpoints, key=lambda x: x[0])[0]
        if current_rmse < worst_rmse:
            should_save = True
    
    if should_save:
        # ç”Ÿæˆæ–°çš„checkpointæ–‡ä»¶è·¯å¾„
        new_checkpoint_path = os.path.join(save_dir, f"{model_session}_epoch_{current_epoch}.pth")
        
        # æ·»åŠ åˆ°åˆ—è¡¨
        best_checkpoints.append((current_rmse, current_epoch, new_checkpoint_path))
        
        # æŒ‰RMSEæ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
        best_checkpoints.sort(key=lambda x: x[0])
        
        # å¦‚æœè¶…è¿‡æœ€å¤§ä¿ç•™æ•°é‡ï¼Œåˆ é™¤æœ€å·®çš„
        while len(best_checkpoints) > max_keep:
            worst_checkpoint = best_checkpoints.pop()  # ç§»é™¤æœ€åä¸€ä¸ªï¼ˆæœ€å·®çš„ï¼‰
            worst_filepath = worst_checkpoint[2]
            
            # åˆ é™¤æ–‡ä»¶
            if os.path.exists(worst_filepath):
                try:
                    os.remove(worst_filepath)
                    print(f"ğŸ—‘ï¸  åˆ é™¤è¾ƒå·®çš„æƒé‡æ–‡ä»¶: {os.path.basename(worst_filepath)} (RMSE: {worst_checkpoint[0]:.4f})")
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤æ–‡ä»¶å¤±è´¥ {worst_filepath}: {e}")
        
        # æ‰“å°å½“å‰ä¿ç•™çš„æœ€å¥½checkpoints
        print(f"ğŸ“ å½“å‰ä¿ç•™çš„æœ€å¥½æƒé‡æ–‡ä»¶:")
        for i, (rmse, epoch, filepath) in enumerate(best_checkpoints):
            print(f"   {i+1}. {os.path.basename(filepath)} - RMSE: {rmse:.4f} (Epoch {epoch})")
    
    return best_checkpoints, should_save


def plot_metrics(epochs, psnr_list, ssim_list, rmse_list, save_dir="."):
    """
    ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡ï¼š
    - PSNR + RMSE åœ¨ä¸€å¼ å›¾
    - SSIM å•ç‹¬ä¸€å¼ å›¾
    """
    os.makedirs(save_dir, exist_ok=True)

    # ---------------- å›¾1: PSNR + RMSE ----------------
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # å·¦è½´: PSNR
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("PSNR â†‘", color='tab:blue')
    ax1.plot(epochs, psnr_list, 'o-', color='tab:blue', label='PSNR â†‘')
    ax1.tick_params(axis='y', labelcolor='tab:blue')
    ax1.grid(True, linestyle='--', alpha=0.5)

    # å³è½´: RMSE
    ax2 = ax1.twinx()
    ax2.set_ylabel("RMSE â†“", color='tab:red')
    ax2.plot(epochs, rmse_list, '^-', color='tab:red', label='RMSE â†“')
    ax2.tick_params(axis='y', labelcolor='tab:red')

    # åˆå¹¶å›¾ä¾‹
    lines, labels = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines + lines2, labels + labels2, loc='best')

    plt.title("Training Metrics (PSNR & RMSE)")
    fig.tight_layout()
    save_path1 = os.path.join(save_dir, "metrics_psnr_rmse.png")
    plt.savefig(save_path1)
    plt.close()
    print(f"PSNR+RMSE å›¾å·²ä¿å­˜åˆ° {save_path1}")

    # ---------------- å›¾2: SSIM ----------------
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, ssim_list, 's-', color='tab:orange', label='SSIM â†‘')
    plt.xlabel("Epoch")
    plt.ylabel("SSIM â†‘")
    plt.title("Training Metrics (SSIM)")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend(loc='best')
    save_path2 = os.path.join(save_dir, "metrics_ssim.png")
    plt.savefig(save_path2)
    plt.close()
    print(f"SSIM å›¾å·²ä¿å­˜åˆ° {save_path2}")




def train():
    # é…ç½®
    opt = Config('config.yml')
    seed_everything(opt.OPTIM.SEED)

    # åˆå§‹åŒ– Accelerator
    if getattr(opt.OPTIM, "WANDB", False):
        accelerator = Accelerator(log_with="wandb")
        accelerator.init_trackers(project_name=getattr(opt.OPTIM, "WANDB_PROJECT", "default_project"))
    else:
        accelerator = Accelerator()

    if accelerator.is_local_main_process:
        os.makedirs(opt.TRAINING.SAVE_DIR, exist_ok=True)

    # æ•°æ®åŠ è½½
    train_dataset = get_data(opt.TRAINING.TRAIN_DIR, opt.MODEL.INPUT, opt.MODEL.TARGET, 'train', opt.TRAINING.ORI,
                             {'w': opt.TRAINING.PS_W, 'h': opt.TRAINING.PS_H})
    trainloader = DataLoader(dataset=train_dataset, batch_size=opt.OPTIM.BATCH_SIZE, shuffle=True,
                             num_workers=opt.TRAINING.NUM_WORKERS, drop_last=False, pin_memory=True)

    val_dataset = get_data(opt.TRAINING.VAL_DIR, opt.MODEL.INPUT, opt.MODEL.TARGET, 'test', opt.TRAINING.ORI,
                           {'w': opt.TRAINING.PS_W, 'h': opt.TRAINING.PS_H})
    testloader = DataLoader(dataset=val_dataset, batch_size=opt.TRAINING.VAL_BATCH_SIZE, shuffle=False,
                            num_workers=opt.TRAINING.VAL_NUM_WORKERS, drop_last=False, pin_memory=True)

    # æ¨¡å‹ä¸æŸå¤±
    model = Model()
    
    # ä»configè·å–æŸå¤±é…ç½®
    loss_config = opt.get_loss_config()
    
    print(f"ä½¿ç”¨æŸå¤±é…ç½®: {opt.LOSS.TYPE}")
    print(f"é…ç½®å‚æ•°: {loss_config}")
    
    # ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼Œé’ˆå¯¹é˜´å½±è¾¹ç¼˜é»‘è¾¹é—®é¢˜ä¼˜åŒ–
    criterion = losses.ReconstructionLoss(**loss_config)
    
    # åˆ›å»ºæŸå¤±æƒé‡è°ƒåº¦å™¨
    loss_scheduler = None
    try:
        if getattr(opt.LOSS_SCHEDULER, 'ENABLE', False):
            loss_scheduler = create_loss_scheduler_from_config(opt, opt.OPTIM.NUM_EPOCHS)
            print("âœ… æŸå¤±æƒé‡è°ƒåº¦å™¨å·²å¯ç”¨")
        else:
            print("â„¹ï¸  æŸå¤±æƒé‡è°ƒåº¦å™¨æœªå¯ç”¨")
    except Exception as e:
        print(f"âš ï¸  æŸå¤±æƒé‡è°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨å›ºå®šæƒé‡: {e}")
        loss_scheduler = None

    # ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
    optimizer_b = optim.AdamW(model.parameters(), lr=opt.OPTIM.LR_INITIAL, betas=(0.9, 0.999),
                              eps=1e-8, weight_decay=0.01)
    scheduler_b = optim.lr_scheduler.CosineAnnealingLR(
        optimizer_b, opt.OPTIM.NUM_EPOCHS, eta_min=opt.OPTIM.LR_MIN
    )

    start_epoch = 1
    best_epoch = 1
    best_rmse = 100

    # æ—©åœå‚æ•°
    patience = getattr(opt.TRAINING, "PATIENCE", 20)
    patience_counter = 0
    
    # ä¿ç•™æœ€å¥½çš„ä¸¤ä¸ªæƒé‡æ–‡ä»¶
    best_checkpoints = []  # å­˜å‚¨æœ€å¥½çš„ä¸¤ä¸ªcheckpointä¿¡æ¯ [(rmse, epoch, filepath), ...]

    # checkpoint
    resume_path = getattr(opt.TRAINING, "RESUME", None)
    if resume_path and os.path.exists(resume_path):
        print(f"=> åŠ è½½checkpoint: {resume_path}")
        checkpoint = torch.load(resume_path, map_location="cpu")

        state_dict = checkpoint['state_dict']
        # å¦‚æœæœ‰ "module." å‰ç¼€ï¼Œåˆ™å»æ‰
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[len("module."):]] = v
            else:
                new_state_dict[k] = v

        # åŠ è½½ä¿®æ”¹åçš„ state_dict
        model.load_state_dict(new_state_dict)

        
        if 'optimizer_state_dict' in checkpoint:
            optimizer_b.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint.get('epoch', 0) + 1
            best_rmse = checkpoint.get('best_rmse', best_rmse)
            best_epoch = checkpoint.get('best_epoch', best_epoch)
            
            # æ¢å¤æŸå¤±è°ƒåº¦å™¨çŠ¶æ€
            if loss_scheduler is not None:
                if 'loss_scheduler_state' in checkpoint:
                    # ä»checkpointæ¢å¤è°ƒåº¦å™¨çŠ¶æ€
                    try:
                        loss_scheduler.current_weights = checkpoint['loss_scheduler_state']['current_weights']
                        loss_scheduler.best_metric = checkpoint['loss_scheduler_state']['best_metric']
                        loss_scheduler.patience_counter = checkpoint['loss_scheduler_state']['patience_counter']
                        loss_scheduler.metric_history = checkpoint['loss_scheduler_state']['metric_history']
                        print(f"âœ… æŸå¤±è°ƒåº¦å™¨çŠ¶æ€å·²ä»checkpointæ¢å¤")
                    except Exception as e:
                        print(f"âš ï¸  æŸå¤±è°ƒåº¦å™¨çŠ¶æ€æ¢å¤å¤±è´¥: {e}")
                else:
                    # æ—§checkpointæ²¡æœ‰è°ƒåº¦å™¨çŠ¶æ€ï¼Œæ ¹æ®å½“å‰epochæ¨æ–­åº”æœ‰çš„æƒé‡
                    print(f"â„¹ï¸  æ—§checkpointæœªåŒ…å«è°ƒåº¦å™¨çŠ¶æ€ï¼Œæ ¹æ®epoch {start_epoch-1} æ¨æ–­æƒé‡...")
                    inferred_weights = loss_scheduler.step(start_epoch - 1, best_rmse)
                    loss_scheduler.current_weights = inferred_weights
                    criterion.update_weights(inferred_weights)
                    print(f"âœ… è°ƒåº¦å™¨æƒé‡å·²æ¨æ–­å¹¶åº”ç”¨")
                    
                    # æ‰“å°æ¨æ–­çš„æƒé‡
                    weights_str = ", ".join([f"{k}:{v:.3f}" for k, v in inferred_weights.items()])
                    print(f"   ğŸ“Š æ¨æ–­çš„æŸå¤±æƒé‡: {weights_str}")
            
            print(f"=> ä»ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ (best_rmse={best_rmse:.4f}, best_epoch={best_epoch})")
        else:
            print("=> åªæ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼Œå°†ä»¥fine-tuneæ¨¡å¼ä»å¤´è®­ç»ƒä¼˜åŒ–å™¨")
    else:
        print("=> æœªæ‰¾åˆ°checkpointï¼Œå°†ä»å¤´è®­ç»ƒ")

    # prepare
    trainloader, testloader = accelerator.prepare(trainloader, testloader)
    model = accelerator.prepare(model)
    optimizer_b, scheduler_b = accelerator.prepare(optimizer_b, scheduler_b)

    size = len(testloader)

    # è®°å½•æŒ‡æ ‡
    epoch_list, psnr_list, ssim_list, rmse_list = [], [], [], []
    
    # åˆ›å»ºæˆ–æ‰“å¼€RMSEè®°å½•æ–‡ä»¶
    rmse_log_path = os.path.join(opt.TRAINING.SAVE_DIR, f"{opt.MODEL.SESSION}_rmse_log.txt")
    if accelerator.is_local_main_process:
        # å¦‚æœæ˜¯æ–­ç‚¹ç»­è®­ï¼Œè¿½åŠ æ¨¡å¼ï¼›å¦åˆ™æ–°å»ºæ–‡ä»¶
        log_mode = 'a' if (resume_path and os.path.exists(resume_path)) else 'w'
        with open(rmse_log_path, log_mode) as f:
            if log_mode == 'w':
                f.write("Epoch,PSNR,SSIM,RMSE\n")  # å†™å…¥è¡¨å¤´
            else:
                f.write(f"\n# Resumed training from epoch {start_epoch}\n")
        print(f"ğŸ“ RMSEæ—¥å¿—å°†ä¿å­˜åˆ°: {rmse_log_path}")

    try:
        for epoch in range(start_epoch, opt.OPTIM.NUM_EPOCHS + 1):
            # æ›´æ–°æŸå¤±æƒé‡ï¼ˆåœ¨æ¯ä¸ªepochå¼€å§‹æ—¶ï¼‰
            if loss_scheduler is not None:
                # å¦‚æœæœ‰éªŒè¯æŒ‡æ ‡å†å²ï¼Œä½¿ç”¨æœ€æ–°çš„RMSE
                validation_metric = best_rmse if epoch > start_epoch else None
                updated_weights = loss_scheduler.step(epoch, validation_metric)
                criterion.update_weights(updated_weights)
            
            model.train()
            for _, data in enumerate(tqdm(trainloader, disable=not accelerator.is_local_main_process)):
                inp, gray, tar = data[0].contiguous(), data[1].contiguous(), data[2]
                optimizer_b.zero_grad()

                # forward
                res = model(gray, inp)

                # ä½¿ç”¨ç®€åŒ–çš„æŸå¤±å‡½æ•°ï¼ˆåªåŒ…å«MSEå’ŒSSIMï¼‰
                train_loss, loss_mse, loss_ssim = criterion(res, tar)

                # backward
                accelerator.backward(train_loss)
                optimizer_b.step()
            scheduler_b.step()

            # éªŒè¯
            if epoch % opt.TRAINING.VAL_AFTER_EVERY == 0:
                model.eval()
                psnr, ssim, rmse = 0, 0, 0
                for _, data in enumerate(tqdm(testloader, disable=not accelerator.is_local_main_process)):
                    inp, gray, tar = data[0].contiguous(), data[1].contiguous(), data[2]
                    with torch.no_grad():
                        res = model(gray, inp)
                    res, tar = accelerator.gather((res, tar))
                    psnr += peak_signal_noise_ratio(res, tar, data_range=1).item()
                    ssim += structural_similarity_index_measure(res, tar, data_range=1).item()
                    rmse += mean_squared_error(torch.mul(res, 255).flatten(),
                                               torch.mul(tar, 255).flatten(),
                                               squared=False).item()
                psnr /= size
                ssim /= size
                rmse /= size

                # è®°å½•
                epoch_list.append(epoch)
                psnr_list.append(psnr)
                ssim_list.append(ssim)
                rmse_list.append(rmse)

                # ç®¡ç†æœ€å¥½çš„æƒé‡æ–‡ä»¶ï¼ˆåªä¿ç•™æœ€å¥½çš„ä¸¤ä¸ªï¼‰
                best_checkpoints, should_save = manage_best_checkpoints(
                    best_checkpoints, rmse, epoch, opt.MODEL.SESSION, opt.TRAINING.SAVE_DIR, max_keep=2
                )
                
                # ä¿å­˜æ¨¡å‹ & æ—©åœ
                if should_save:
                    checkpoint_data = {
                        'state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer_b.state_dict(),
                        'epoch': epoch,
                        'best_rmse': min(rmse, best_rmse),
                        'best_epoch': epoch if rmse < best_rmse else best_epoch,
                    }
                    
                    # ä¿å­˜æŸå¤±è°ƒåº¦å™¨çŠ¶æ€
                    if loss_scheduler is not None:
                        checkpoint_data['loss_scheduler_state'] = {
                            'current_weights': loss_scheduler.current_weights,
                            'best_metric': loss_scheduler.best_metric,
                            'patience_counter': loss_scheduler.patience_counter,
                            'metric_history': loss_scheduler.metric_history
                        }
                    
                    save_checkpoint(checkpoint_data, epoch, opt.MODEL.SESSION, opt.TRAINING.SAVE_DIR)
                
                # æ›´æ–°æœ€ä½³è®°å½•
                if rmse < best_rmse:
                    best_epoch = epoch
                    best_rmse = rmse
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"æ—©åœè§¦å‘ï¼è¿ç»­ {patience} ä¸ªéªŒè¯å‘¨æœŸæ— æå‡ (best RMSE={best_rmse:.4f}, best epoch={best_epoch})")
                        raise KeyboardInterrupt

                # æ—¥å¿—ï¼ˆä»…åœ¨å¯ç”¨ wandb æ—¶æ‰æ‰§è¡Œï¼‰
                if getattr(opt.OPTIM, "WANDB", False):
                    accelerator.log({"PSNR": psnr, "SSIM": ssim, "RMSE": rmse}, step=epoch)

                # æ§åˆ¶å°è¾“å‡º
                if accelerator.is_local_main_process:
                    output_str = (f"epoch: {epoch}, RMSE:{rmse:.4f}, PSNR: {psnr:.4f}, SSIM: {ssim:.4f}, "
                                f"best RMSE: {best_rmse:.4f}, best epoch: {best_epoch}")
                    
                    # å¦‚æœå¯ç”¨äº†æŸå¤±è°ƒåº¦å™¨ï¼Œæ˜¾ç¤ºå½“å‰æƒé‡
                    if loss_scheduler is not None and epoch % 10 == 0:
                        current_weights = criterion.get_weights()
                        weights_str = ", ".join([f"{k}:{v:.3f}" for k, v in current_weights.items()])
                        output_str += f"\n   ğŸ“Š å½“å‰æŸå¤±æƒé‡: {weights_str}"
                    
                    print(output_str)
                    
                    # è®°å½•RMSEåˆ°æ–‡ä»¶
                    with open(rmse_log_path, 'a') as f:
                        f.write(f"{epoch},{psnr:.6f},{ssim:.6f},{rmse:.6f}\n")

    except KeyboardInterrupt:
        print("\n==> è®­ç»ƒåœæ­¢ï¼Œå¼€å§‹ä¿å­˜æŒ‡æ ‡å›¾ ...")

    # æ˜¾ç¤ºæœ€ç»ˆä¿ç•™çš„æƒé‡æ–‡ä»¶
    if accelerator.is_local_main_process and len(best_checkpoints) > 0:
        print(f"\nğŸ† è®­ç»ƒå®Œæˆï¼æœ€ç»ˆä¿ç•™çš„æœ€å¥½æƒé‡æ–‡ä»¶:")
        for i, (rmse, epoch, filepath) in enumerate(best_checkpoints):
            status = "ğŸ¥‡ æœ€ä½³" if i == 0 else "ğŸ¥ˆ æ¬¡ä½³"
            print(f"   {status}: {os.path.basename(filepath)} - RMSE: {rmse:.4f} (Epoch {epoch})")
        
        # æ¨èä½¿ç”¨æœ€ä½³æƒé‡è¿›è¡Œæµ‹è¯•
        if len(best_checkpoints) > 0:
            best_checkpoint_path = best_checkpoints[0][2]
            print(f"\nğŸ’¡ æ¨èä½¿ç”¨æœ€ä½³æƒé‡è¿›è¡Œæµ‹è¯•:")
            print(f"   python test.py TESTING.WEIGHT \"{best_checkpoint_path}\"")

    # ç»˜åˆ¶æŒ‡æ ‡å›¾
    if len(epoch_list) > 0:
        plot_metrics(epoch_list, psnr_list, ssim_list, rmse_list,
             save_dir=opt.TRAINING.SAVE_DIR)

    accelerator.end_training()


if __name__ == '__main__':
    train()
