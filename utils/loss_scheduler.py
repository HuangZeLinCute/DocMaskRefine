"""
æŸå¤±å‡½æ•°è°ƒæ•´å™¨ - åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å„ä¸ªæŸå¤±åˆ†é‡çš„æƒé‡
æ”¯æŒå¤šç§è°ƒæ•´ç­–ç•¥ï¼šçº¿æ€§ã€ä½™å¼¦ã€æŒ‡æ•°ã€é˜¶æ¢¯å¼ç­‰
"""

import torch
import math
from typing import Dict, List, Union, Optional
from enum import Enum


class ScheduleType(Enum):
    """è°ƒåº¦å™¨ç±»å‹æšä¸¾"""
    LINEAR = "linear"
    COSINE = "cosine"
    EXPONENTIAL = "exponential"
    STEP = "step"
    WARMUP_COSINE = "warmup_cosine"
    ADAPTIVE = "adaptive"
    CONSTANT = "constant"


class LossWeightScheduler:
    """
    æŸå¤±æƒé‡è°ƒåº¦å™¨
    
    æ”¯æŒä¸ºä¸åŒçš„æŸå¤±åˆ†é‡è®¾ç½®ä¸åŒçš„è°ƒåº¦ç­–ç•¥ï¼Œ
    å¯ä»¥æ ¹æ®è®­ç»ƒè¿›åº¦ã€éªŒè¯æŒ‡æ ‡ç­‰åŠ¨æ€è°ƒæ•´æƒé‡
    """
    
    def __init__(self, 
                 total_epochs: int,
                 loss_configs: Dict[str, Dict],
                 adaptive_patience: int = 5,
                 adaptive_factor: float = 0.8,
                 verbose: bool = True):
        """
        Args:
            total_epochs: æ€»è®­ç»ƒè½®æ•°
            loss_configs: æŸå¤±é…ç½®å­—å…¸ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                {
                    "mse": {
                        "schedule_type": "constant",
                        "initial_weight": 1.0,
                        "final_weight": 1.0
                    },
                    "ssim": {
                        "schedule_type": "warmup_cosine", 
                        "initial_weight": 0.1,
                        "final_weight": 0.5,
                        "warmup_epochs": 10
                    },
                    "edge": {
                        "schedule_type": "linear",
                        "initial_weight": 0.3,
                        "final_weight": 0.8,
                        "start_epoch": 5
                    }
                }
            adaptive_patience: è‡ªé€‚åº”è°ƒæ•´çš„è€å¿ƒå€¼
            adaptive_factor: è‡ªé€‚åº”è°ƒæ•´å› å­
            verbose: æ˜¯å¦æ‰“å°è°ƒæ•´ä¿¡æ¯
        """
        self.total_epochs = total_epochs
        self.loss_configs = loss_configs
        self.adaptive_patience = adaptive_patience
        self.adaptive_factor = adaptive_factor
        self.verbose = verbose
        
        # å½“å‰æƒé‡
        self.current_weights = {}
        
        # è‡ªé€‚åº”è°ƒæ•´ç›¸å…³
        self.best_metric = float('inf')
        self.patience_counter = 0
        self.metric_history = []
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        if self.verbose:
            print("ğŸ¯ æŸå¤±æƒé‡è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
            self._print_schedule_info()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ‰€æœ‰æŸå¤±æƒé‡"""
        for loss_name, config in self.loss_configs.items():
            self.current_weights[loss_name] = config.get("initial_weight", 1.0)
    
    def _print_schedule_info(self):
        """æ‰“å°è°ƒåº¦ä¿¡æ¯"""
        print("\nğŸ“‹ æŸå¤±æƒé‡è°ƒåº¦é…ç½®:")
        for loss_name, config in self.loss_configs.items():
            schedule_type = config.get("schedule_type", "constant")
            initial = config.get("initial_weight", 1.0)
            final = config.get("final_weight", initial)
            print(f"   {loss_name:12s}: {schedule_type:15s} {initial:.3f} â†’ {final:.3f}")
        print()
    
    def get_current_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡"""
        return self.current_weights.copy()
    
    def step(self, epoch: int, validation_metric: Optional[float] = None) -> Dict[str, float]:
        """
        æ›´æ–°æƒé‡
        
        Args:
            epoch: å½“å‰epoch (1-based)
            validation_metric: éªŒè¯æŒ‡æ ‡ (å¦‚RMSEï¼Œè¶Šå°è¶Šå¥½)
            
        Returns:
            updated_weights: æ›´æ–°åçš„æƒé‡å­—å…¸
        """
        # æ›´æ–°å„ä¸ªæŸå¤±æƒé‡
        for loss_name, config in self.loss_configs.items():
            new_weight = self._compute_weight(loss_name, config, epoch)
            self.current_weights[loss_name] = new_weight
        
        # è‡ªé€‚åº”è°ƒæ•´
        if validation_metric is not None:
            self._adaptive_adjustment(validation_metric, epoch)
        
        # æ‰“å°æƒé‡å˜åŒ–
        if self.verbose and epoch % 10 == 0:
            self._print_current_weights(epoch)
        
        return self.get_current_weights()
    
    def _compute_weight(self, loss_name: str, config: Dict, epoch: int) -> float:
        """è®¡ç®—ç‰¹å®šæŸå¤±çš„æƒé‡"""
        schedule_type = config.get("schedule_type", "constant")
        initial_weight = config.get("initial_weight", 1.0)
        final_weight = config.get("final_weight", initial_weight)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼€å§‹epoché™åˆ¶
        start_epoch = config.get("start_epoch", 1)
        if epoch < start_epoch:
            return 0.0
        
        # è°ƒæ•´epochä¸ºç›¸å¯¹äºstart_epochçš„å€¼
        relative_epoch = epoch - start_epoch + 1
        relative_total = self.total_epochs - start_epoch + 1
        
        if schedule_type == ScheduleType.CONSTANT.value:
            return initial_weight
            
        elif schedule_type == ScheduleType.LINEAR.value:
            progress = min(relative_epoch / relative_total, 1.0)
            return initial_weight + (final_weight - initial_weight) * progress
            
        elif schedule_type == ScheduleType.COSINE.value:
            progress = min(relative_epoch / relative_total, 1.0)
            cosine_progress = 0.5 * (1 + math.cos(math.pi * progress))
            return final_weight + (initial_weight - final_weight) * cosine_progress
            
        elif schedule_type == ScheduleType.EXPONENTIAL.value:
            decay_rate = config.get("decay_rate", 0.95)
            return initial_weight * (decay_rate ** (relative_epoch - 1))
            
        elif schedule_type == ScheduleType.STEP.value:
            step_size = config.get("step_size", 50)
            step_gamma = config.get("step_gamma", 0.5)
            steps = (relative_epoch - 1) // step_size
            return initial_weight * (step_gamma ** steps)
            
        elif schedule_type == ScheduleType.WARMUP_COSINE.value:
            warmup_epochs = config.get("warmup_epochs", 10)
            
            if relative_epoch <= warmup_epochs:
                # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿
                progress = relative_epoch / warmup_epochs
                return initial_weight * progress
            else:
                # Cosineé˜¶æ®µ
                cosine_epochs = relative_total - warmup_epochs
                cosine_progress = (relative_epoch - warmup_epochs) / cosine_epochs
                cosine_progress = min(cosine_progress, 1.0)
                cosine_factor = 0.5 * (1 + math.cos(math.pi * cosine_progress))
                return final_weight + (initial_weight - final_weight) * cosine_factor
                
        else:
            return initial_weight
    
    def _adaptive_adjustment(self, validation_metric: float, epoch: int):
        """åŸºäºéªŒè¯æŒ‡æ ‡çš„è‡ªé€‚åº”è°ƒæ•´"""
        self.metric_history.append(validation_metric)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        if validation_metric < self.best_metric:
            self.best_metric = validation_metric
            self.patience_counter = 0
        else:
            self.patience_counter += 1
        
        # å¦‚æœè¿ç»­å¤šä¸ªepochæ²¡æœ‰æ”¹å–„ï¼Œè°ƒæ•´æƒé‡
        if self.patience_counter >= self.adaptive_patience:
            self._perform_adaptive_adjustment(epoch)
            self.patience_counter = 0
    
    def _perform_adaptive_adjustment(self, epoch: int):
        """æ‰§è¡Œè‡ªé€‚åº”æƒé‡è°ƒæ•´"""
        if self.verbose:
            print(f"\nğŸ”„ Epoch {epoch}: æ‰§è¡Œè‡ªé€‚åº”æƒé‡è°ƒæ•´")
        
        # ç­–ç•¥1: å¢å¼ºè¾¹ç¼˜å’Œæ¢¯åº¦æŸå¤±ï¼ˆé€šå¸¸æœ‰åŠ©äºç»†èŠ‚ï¼‰
        if "edge" in self.current_weights:
            old_weight = self.current_weights["edge"]
            self.current_weights["edge"] = min(old_weight * 1.2, 2.0)
            if self.verbose:
                print(f"   edgeæƒé‡: {old_weight:.3f} â†’ {self.current_weights['edge']:.3f}")
        
        if "gradient" in self.current_weights:
            old_weight = self.current_weights["gradient"]
            self.current_weights["gradient"] = min(old_weight * 1.1, 1.0)
            if self.verbose:
                print(f"   gradientæƒé‡: {old_weight:.3f} â†’ {self.current_weights['gradient']:.3f}")
        
        # ç­–ç•¥2: é€‚åº¦é™ä½æ„ŸçŸ¥æŸå¤±ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
        if "perceptual" in self.current_weights:
            old_weight = self.current_weights["perceptual"]
            self.current_weights["perceptual"] = max(old_weight * self.adaptive_factor, 0.05)
            if self.verbose:
                print(f"   perceptualæƒé‡: {old_weight:.3f} â†’ {self.current_weights['perceptual']:.3f}")
    
    def _print_current_weights(self, epoch: int):
        """æ‰“å°å½“å‰æƒé‡"""
        print(f"\nğŸ“Š Epoch {epoch} å½“å‰æŸå¤±æƒé‡:")
        for loss_name, weight in self.current_weights.items():
            print(f"   {loss_name:12s}: {weight:.4f}")
    
    def get_schedule_summary(self) -> str:
        """è·å–è°ƒåº¦å™¨æ‘˜è¦ä¿¡æ¯"""
        summary = "Loss Weight Scheduler Summary:\n"
        summary += f"Total Epochs: {self.total_epochs}\n"
        summary += f"Adaptive Patience: {self.adaptive_patience}\n"
        summary += "Loss Configurations:\n"
        
        for loss_name, config in self.loss_configs.items():
            schedule_type = config.get("schedule_type", "constant")
            initial = config.get("initial_weight", 1.0)
            final = config.get("final_weight", initial)
            summary += f"  {loss_name}: {schedule_type} ({initial:.3f} â†’ {final:.3f})\n"
        
        return summary
    
    def save_state(self, filepath: str):
        """ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€"""
        state = {
            'current_weights': self.current_weights,
            'best_metric': self.best_metric,
            'patience_counter': self.patience_counter,
            'metric_history': self.metric_history
        }
        torch.save(state, filepath)
    
    def load_state(self, filepath: str):
        """åŠ è½½è°ƒåº¦å™¨çŠ¶æ€"""
        state = torch.load(filepath, map_location='cpu')
        self.current_weights = state.get('current_weights', {})
        self.best_metric = state.get('best_metric', float('inf'))
        self.patience_counter = state.get('patience_counter', 0)
        self.metric_history = state.get('metric_history', [])


class PresetLossSchedulers:
    """é¢„è®¾çš„æŸå¤±è°ƒåº¦å™¨é…ç½®"""
    
    @staticmethod
    def get_shadow_removal_config(total_epochs: int) -> Dict[str, Dict]:
        """
        é˜´å½±å»é™¤ä»»åŠ¡çš„æ¨èé…ç½®
        
        ç­–ç•¥ï¼š
        1. MSEä¿æŒæ’å®šä½œä¸ºåŸºç¡€
        2. SSIMé€æ¸å¢å¼ºï¼Œä¿æŒç»“æ„
        3. Edgeæƒé‡å…ˆå¢åå‡ï¼Œé‡ç‚¹å¤„ç†è¾¹ç¼˜
        4. Gradienté€æ¸å¢å¼ºï¼Œç¡®ä¿å¹³æ»‘
        5. Boundaryåœ¨ä¸­æœŸè¾¾åˆ°å³°å€¼
        6. Perceptualå’ŒHistogramä¿æŒé€‚ä¸­
        """
        return {
            "mse": {
                "schedule_type": "constant",
                "initial_weight": 1.0,
                "final_weight": 1.0
            },
            "ssim": {
                "schedule_type": "warmup_cosine",
                "initial_weight": 0.1,
                "final_weight": 0.4,
                "warmup_epochs": max(10, total_epochs // 20)
            },
            "edge": {
                "schedule_type": "cosine",
                "initial_weight": 0.3,
                "final_weight": 0.8,
                "start_epoch": 5
            },
            "gradient": {
                "schedule_type": "linear",
                "initial_weight": 0.2,
                "final_weight": 0.5
            },
            "boundary": {
                "schedule_type": "warmup_cosine",
                "initial_weight": 0.3,
                "final_weight": 0.6,
                "warmup_epochs": max(15, total_epochs // 15)
            },
            "transparency": {
                "schedule_type": "linear",
                "initial_weight": 0.1,
                "final_weight": 0.3,
                "start_epoch": 10
            },
            "perceptual": {
                "schedule_type": "constant",
                "initial_weight": 0.2,
                "final_weight": 0.2
            },
            "histogram": {
                "schedule_type": "linear",
                "initial_weight": 0.1,
                "final_weight": 0.25,
                "start_epoch": 20
            }
        }
    
    @staticmethod
    def get_fine_tuning_config(total_epochs: int) -> Dict[str, Dict]:
        """
        å¾®è°ƒé˜¶æ®µçš„é…ç½® - æ›´æ³¨é‡ç»†èŠ‚ä¼˜åŒ–
        """
        return {
            "mse": {
                "schedule_type": "exponential",
                "initial_weight": 1.0,
                "decay_rate": 0.98
            },
            "ssim": {
                "schedule_type": "linear",
                "initial_weight": 0.3,
                "final_weight": 0.6
            },
            "edge": {
                "schedule_type": "linear",
                "initial_weight": 0.5,
                "final_weight": 1.0
            },
            "gradient": {
                "schedule_type": "linear",
                "initial_weight": 0.3,
                "final_weight": 0.7
            },
            "boundary": {
                "schedule_type": "constant",
                "initial_weight": 0.8,
                "final_weight": 0.8
            },
            "transparency": {
                "schedule_type": "linear",
                "initial_weight": 0.2,
                "final_weight": 0.4
            },
            "perceptual": {
                "schedule_type": "exponential",
                "initial_weight": 0.3,
                "decay_rate": 0.95
            },
            "histogram": {
                "schedule_type": "linear",
                "initial_weight": 0.2,
                "final_weight": 0.3
            }
        }
    
    @staticmethod
    def get_aggressive_config(total_epochs: int) -> Dict[str, Dict]:
        """
        æ¿€è¿›é…ç½® - å¿«é€Ÿæ”¶æ•›ï¼Œé€‚åˆæ•°æ®å……è¶³çš„æƒ…å†µ
        """
        return {
            "mse": {
                "schedule_type": "constant",
                "initial_weight": 1.0,
                "final_weight": 1.0
            },
            "ssim": {
                "schedule_type": "linear",
                "initial_weight": 0.5,
                "final_weight": 0.8
            },
            "edge": {
                "schedule_type": "linear",
                "initial_weight": 0.8,
                "final_weight": 1.2
            },
            "gradient": {
                "schedule_type": "linear",
                "initial_weight": 0.4,
                "final_weight": 0.8
            },
            "boundary": {
                "schedule_type": "linear",
                "initial_weight": 0.6,
                "final_weight": 1.0
            },
            "transparency": {
                "schedule_type": "linear",
                "initial_weight": 0.3,
                "final_weight": 0.5
            },
            "perceptual": {
                "schedule_type": "constant",
                "initial_weight": 0.3,
                "final_weight": 0.3
            },
            "histogram": {
                "schedule_type": "linear",
                "initial_weight": 0.2,
                "final_weight": 0.4
            }
        }


def create_loss_scheduler_from_config(config, total_epochs: int) -> LossWeightScheduler:
    """
    ä»é…ç½®æ–‡ä»¶åˆ›å»ºæŸå¤±è°ƒåº¦å™¨
    
    Args:
        config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«LOSS_SCHEDULERéƒ¨åˆ†
        total_epochs: æ€»è®­ç»ƒè½®æ•°
        
    Returns:
        LossWeightSchedulerå®ä¾‹
    """
    try:
        # è·å–è°ƒåº¦å™¨é…ç½®
        scheduler_config = config.LOSS_SCHEDULER
        
        # è·å–é¢„è®¾ç±»å‹
        preset_type = getattr(scheduler_config, 'PRESET', 'shadow_removal')
        
        # æ ¹æ®é¢„è®¾ç±»å‹è·å–é…ç½®
        if preset_type == 'shadow_removal':
            loss_configs = PresetLossSchedulers.get_shadow_removal_config(total_epochs)
        elif preset_type == 'fine_tuning':
            loss_configs = PresetLossSchedulers.get_fine_tuning_config(total_epochs)
        elif preset_type == 'aggressive':
            loss_configs = PresetLossSchedulers.get_aggressive_config(total_epochs)
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            print(f"âš ï¸  æœªçŸ¥çš„é¢„è®¾ç±»å‹ '{preset_type}'ï¼Œä½¿ç”¨é»˜è®¤ shadow_removal é…ç½®")
            loss_configs = PresetLossSchedulers.get_shadow_removal_config(total_epochs)
        
        # æš‚æ—¶è·³è¿‡ OVERRIDES å¤„ç†ï¼Œé¿å…é…ç½®é”™è¯¯
        # TODO: åç»­å¯ä»¥æ·»åŠ æ›´å®Œå–„çš„è¦†ç›–æœºåˆ¶
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = LossWeightScheduler(
            total_epochs=total_epochs,
            loss_configs=loss_configs,
            adaptive_patience=getattr(scheduler_config, 'ADAPTIVE_PATIENCE', 5),
            adaptive_factor=getattr(scheduler_config, 'ADAPTIVE_FACTOR', 0.8),
            verbose=getattr(scheduler_config, 'VERBOSE', True)
        )
        
        return scheduler
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºæŸå¤±è°ƒåº¦å™¨å¤±è´¥: {e}")
        # è¿”å›ä¸€ä¸ªç®€å•çš„é»˜è®¤è°ƒåº¦å™¨
        loss_configs = PresetLossSchedulers.get_shadow_removal_config(total_epochs)
        return LossWeightScheduler(
            total_epochs=total_epochs,
            loss_configs=loss_configs,
            adaptive_patience=5,
            adaptive_factor=0.8,
            verbose=True
        )


if __name__ == '__main__':
    # æµ‹è¯•æŸå¤±è°ƒåº¦å™¨
    print("ğŸ§ª æµ‹è¯•æŸå¤±æƒé‡è°ƒåº¦å™¨")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    loss_configs = PresetLossSchedulers.get_shadow_removal_config(100)
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = LossWeightScheduler(
        total_epochs=100,
        loss_configs=loss_configs,
        verbose=True
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("\nğŸš€ æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹:")
    test_epochs = [1, 10, 25, 50, 75, 100]
    test_metrics = [0.5, 0.3, 0.25, 0.22, 0.21, 0.20]
    
    for epoch, metric in zip(test_epochs, test_metrics):
        weights = scheduler.step(epoch, metric)
        print(f"\nEpoch {epoch:3d} (RMSE: {metric:.3f}):")
        for loss_name, weight in weights.items():
            print(f"  {loss_name:12s}: {weight:.4f}")